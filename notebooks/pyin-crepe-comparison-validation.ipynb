{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as display\n",
    "import numpy as np\n",
    "import json\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from random import randrange\n",
    "\n",
    "import torchcrepe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = '/home/purnima/appdir/Github/DATA/NSynth/' #Path to your Downloaded NSynth dataset\n",
    "train_data_dir = os.path.join(base_data_dir,'nsynth-train', 'audio')\n",
    "test_data_dir = os.path.join(base_data_dir,'nsynth-test', 'audio')\n",
    "validate_data_dir = os.path.join(base_data_dir,'nsynth-valid', 'audio')\n",
    "\n",
    "labels_dir = '/home/purnima/appdir/Github/DATA/NSynth'\n",
    "labels_file_name = 'examples-subset-full-acoustic-3000.json'\n",
    "\n",
    "labels_train_dir = os.path.join(labels_dir,'nsynth-train', labels_file_name)\n",
    "labels_test_dir = os.path.join(labels_dir,'nsynth-test', labels_file_name)\n",
    "labels_validate_dir = os.path.join(labels_dir,'nsynth-valid', labels_file_name)\n",
    "\n",
    "sample_rate = 16000\n",
    "sample_length = 2048\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is different than the one in the code base. This will be used to source dataset for pYIN and CREPE\n",
    "# which need only raw audio data and not the STFT channels.\n",
    "class NSynthDataSet_RawAudio(Dataset):\n",
    "    def __init__(self, meta_data_file, audio_dir, lower_pitch_limit, upper_pitch_limit, sr=16000):\n",
    "        self.meta_data_file = meta_data_file\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sr = sr\n",
    "        \n",
    "        with open(meta_data_file) as f:\n",
    "            params = json.load(f)\n",
    "            self.nsynth_meta_df = pd.DataFrame.from_dict(params)\n",
    "            self.nsynth_meta_df = self.nsynth_meta_df.transpose()\n",
    "            self.nsynth_meta_df = self.nsynth_meta_df[self.nsynth_meta_df['instrument_family_str'] == 'guitar']\n",
    "            self.nsynth_meta_df = self.nsynth_meta_df[(self.nsynth_meta_df['pitch'] >= lower_pitch_limit) \\\n",
    "                                                      & (self.nsynth_meta_df['pitch'] < upper_pitch_limit)]\n",
    "            \n",
    "            self.nsynth_meta_df['part'] = 1\n",
    "            nsynth_meta_df_2 = self.nsynth_meta_df.copy(deep=True)\n",
    "            nsynth_meta_df_2['part'] = 2\n",
    "            nsynth_meta_df_2.index = nsynth_meta_df_2.index + '-2'\n",
    "            nsynth_meta_df_3 = self.nsynth_meta_df.copy(deep=True)\n",
    "            nsynth_meta_df_3['part'] = 3\n",
    "            nsynth_meta_df_3.index = nsynth_meta_df_3.index + '-3'\n",
    "            nsynth_meta_df_4 = self.nsynth_meta_df.copy(deep=True)\n",
    "            nsynth_meta_df_4['part'] = 4\n",
    "            nsynth_meta_df_4.index = nsynth_meta_df_4.index + '-4'\n",
    "            self.nsynth_meta_df = pd.concat([self.nsynth_meta_df, nsynth_meta_df_2, nsynth_meta_df_3, nsynth_meta_df_4])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.nsynth_meta_df.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx): #In case we get [0] instead of 0\n",
    "            idx = idx.tolist()\n",
    "        audio_file_name = self.nsynth_meta_df.iloc[idx].note_str + '.wav'\n",
    "        audio_pitch = self.nsynth_meta_df.iloc[idx].pitch\n",
    "        audio_data, _ = librosa.load(os.path.join(self.audio_dir, audio_file_name), sr=self.sr)\n",
    "        \n",
    "        mult = 0.25 + ((self.nsynth_meta_df.iloc[idx].part - 1) * 0.5)\n",
    "        start_location = int(16000 * mult)\n",
    "        \n",
    "        audio_data = audio_data[start_location:start_location+sample_length]\n",
    "        return audio_data, audio_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS for MIDI 21 to 40\n",
    "lower_freq_validate_ds = NSynthDataSet_RawAudio(meta_data_file=labels_validate_dir, audio_dir=validate_data_dir, lower_pitch_limit=21, upper_pitch_limit=41, sr=sample_rate)\n",
    "lower_freq_validate_loader = torch.utils.data.DataLoader(lower_freq_validate_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "#DS for MIDI 41 to 80\n",
    "upper_freq_validate_ds = NSynthDataSet_RawAudio(meta_data_file=labels_validate_dir, audio_dir=validate_data_dir, lower_pitch_limit=41, upper_pitch_limit=81, sr=sample_rate)\n",
    "upper_freq_validate_loader = torch.utils.data.DataLoader(upper_freq_validate_ds, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pyin(dl):\n",
    "    num_correct_pyin = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(dl):\n",
    "            data = data[0].float()\n",
    "            data = data.numpy()\n",
    "            \n",
    "            pyin_f0, _, _ = librosa.pyin(data, fmin=librosa.note_to_hz('A0'), fmax=librosa.note_to_hz('A5'), sr=16000, frame_length=1000, hop_length=1000)\n",
    "            pyin_f0 = np.nan_to_num(pyin_f0)\n",
    "            pyin_f0 = np.max(pyin_f0)\n",
    "            \n",
    "            prediction = librosa.core.hz_to_midi(pyin_f0)\n",
    "            if target - 0.5 < prediction < target + 0.5: #Tolerance of 0.5\n",
    "                num_correct_pyin += 1\n",
    "                \n",
    "            \n",
    "    print('Total correct = ', num_correct_pyin, ' i.e.{:.2f}%'.format(num_correct_pyin * 100/(len(dl.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct =  749  i.e.55.24%\n",
      "Total correct =  2517  i.e.93.78%\n"
     ]
    }
   ],
   "source": [
    "validate_pyin(lower_freq_validate_loader) #pYIN algo takes time to execute. Around 5-10 mins.\n",
    "validate_pyin(upper_freq_validate_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_crepe(dl):\n",
    "    #crepe\n",
    "    num_correct_crepe = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(dl):\n",
    "            data = data[0].float()\n",
    "            data = data.numpy()\n",
    "            \n",
    "            data_tensor = torch.from_numpy(data).view(1, -1)\n",
    "            crepe_f0 = torchcrepe.predict(data_tensor,\n",
    "                           16000,\n",
    "                           2048,\n",
    "                           40,\n",
    "                           3400,\n",
    "                           'full',\n",
    "                           batch_size=1,\n",
    "                           device=device)\n",
    "            crepe_f0 = np.mean(crepe_f0.numpy())\n",
    "            \n",
    "            prediction = librosa.core.hz_to_midi(crepe_f0)\n",
    "            if target - 0.5 < prediction < target + 0.5: #Tolerance of 0.5\n",
    "                num_correct_crepe += 1\n",
    "            \n",
    "    print('Total correct = ', num_correct_crepe, ' i.e.{:.2f}%'.format(num_correct_crepe * 100/(len(dl.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct =  240  i.e.17.70%\n"
     ]
    }
   ],
   "source": [
    "validate_crepe(lower_freq_validate_loader)\n",
    "validate_crepe(upper_freq_validate_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
